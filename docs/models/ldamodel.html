

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>models.ldamodel – Latent Dirichlet Allocation &mdash; gensim</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.8.4',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="gensim" href="../index.html" />
    <link rel="up" title="API Reference" href="../apiref.html" />
    <link rel="next" title="models.lsimodel – Latent Semantic Indexing" href="lsimodel.html" />
    <link rel="prev" title="corpora.indexedcorpus – Random access to corpus documents" href="../corpora/indexedcorpus.html" />
     

	<!-- twitter search widget
	    <script type="text/javascript" src="_static/widget.js"></script>
	-->
	<meta property="og:title" content="#gensim" />
	<meta property="og:description" content="Efficient topic modelling in Python" />

	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-24066335-1']);
		_gaq.push(['_trackPageview']);

		(function() {
		var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>

    


  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="lsimodel.html" title="models.lsimodel – Latent Semantic Indexing"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../corpora/indexedcorpus.html" title="corpora.indexedcorpus – Random access to corpus documents"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Gensim home</a>|&nbsp;</li>
        <li><a href="../tutorial.html">Tutorials</a>|&nbsp;</li>
        <li><a href="http://groups.google.com/group/gensim">Support</a>|&nbsp;</li>
        <li><a href="https://github.com/piskvorky/gensim/wiki">Contribute</a>|&nbsp;</li>
        <li><a href="../apiref.html">API reference</a>&raquo;</li>

          <li><a href="../apiref.html" accesskey="U">API Reference</a> &raquo;</li> 
      </ul>
    </div>

    
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="../corpora/indexedcorpus.html"
                        title="previous chapter"><tt class="docutils literal docutils literal docutils literal"><span class="pre">corpora.indexedcorpus</span></tt> &#8211; Random access to corpus documents</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="lsimodel.html"
                        title="next chapter"><tt class="docutils literal"><span class="pre">models.lsimodel</span></tt> &#8211; Latent Semantic Indexing</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" size="24" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
    



    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="module-gensim.models.ldamodel">
<span id="models-ldamodel-latent-dirichlet-allocation"></span><h1><tt class="xref py py-mod docutils literal"><span class="pre">models.ldamodel</span></tt> &#8211; Latent Dirichlet Allocation<a class="headerlink" href="#module-gensim.models.ldamodel" title="Permalink to this headline">¶</a></h1>
<p>This module encapsulates functionality for the Latent Dirichlet Allocation algorithm.</p>
<p>It allows both model estimation from a training corpus and inference of topic
distribution on new, unseen documents.</p>
<p>The core estimation code is directly adapted from the <cite>onlineldavb.py</cite> script
by M. Hoffman <a class="footnote-reference" href="#id2" id="id1">[1]</a>, see
<strong>Hoffman, Blei, Bach: Online Learning for Latent Dirichlet Allocation, NIPS 2010.</strong></p>
<p>The algorithm:</p>
<blockquote>
<div><ul class="simple">
<li>is <strong>streamed</strong>: training documents come in sequentially, no random access,</li>
<li>runs in <strong>constant memory</strong> w.r.t. the number of documents: size of the
training corpus does not affect memory footprint, and</li>
<li>is <strong>distributed</strong>: makes use of a cluster of machines, if available, to
speed up model estimation.</li>
</ul>
</div></blockquote>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td><a class="reference external" href="http://www.cs.princeton.edu/~mdhoffma">http://www.cs.princeton.edu/~mdhoffma</a></td></tr>
</tbody>
</table>
<dl class="class">
<dt id="gensim.models.ldamodel.LdaModel">
<em class="property">class </em><tt class="descclassname">gensim.models.ldamodel.</tt><tt class="descname">LdaModel</tt><big>(</big><em>corpus=None</em>, <em>num_topics=100</em>, <em>id2word=None</em>, <em>distributed=False</em>, <em>chunksize=2000</em>, <em>passes=1</em>, <em>update_every=1</em>, <em>alpha=None</em>, <em>eta=None</em>, <em>decay=0.5</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The constructor estimates Latent Dirichlet Allocation model parameters based
on a training corpus:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>You can then infer topic distributions on new, unseen documents, with</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">doc_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">]</span>
</pre></div>
</div>
<p>The model can be updated (trained) with new documents via</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">other_corpus</span><span class="p">)</span>
</pre></div>
</div>
<p>Model persistency is achieved through its <cite>load</cite>/<cite>save</cite> methods.</p>
<p><cite>num_topics</cite> is the number of requested latent topics to be extracted from
the training corpus.</p>
<p><cite>id2word</cite> is a mapping from word ids (integers) to words (strings). It is
used to determine the vocabulary size, as well as for debugging and topic
printing.</p>
<p><cite>alpha</cite> and <cite>eta</cite> are hyperparameters that affect sparsity of the document-topic
(theta) and topic-word (lambda) distributions. Both default to a symmetric
1.0/num_topics (but can be set to a vector, for asymmetric priors).</p>
<p>Turn on <cite>distributed</cite> to force distributed computing (see the web tutorial
on how to set up a cluster of machines for gensim).</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">]</span> <span class="c"># get topic probability distribution for a document</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">corpus2</span><span class="p">)</span> <span class="c"># update the LDA model with additional documents</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">]</span>
</pre></div>
</div>
<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.bound">
<tt class="descname">bound</tt><big>(</big><em>corpus</em>, <em>gamma=None</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.bound" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate the variational bound of documents from <cite>corpus</cite>.</p>
<p><cite>gamma</cite> are the variational parameters on topic weights (one for each
document in <cite>corpus</cite>). If not supplied, will be automatically inferred
from the model.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.clear">
<tt class="descname">clear</tt><big>(</big><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear model state (free up some memory). Used in the distributed algo.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.do_estep">
<tt class="descname">do_estep</tt><big>(</big><em>chunk</em>, <em>state=None</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.do_estep" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform inference on a chunk of documents, and accumulate the collected
sufficient statistics in <cite>state</cite> (or <cite>self.state</cite> if None).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.do_mstep">
<tt class="descname">do_mstep</tt><big>(</big><em>rho</em>, <em>other</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.do_mstep" title="Permalink to this definition">¶</a></dt>
<dd><p>M step: use linear interpolation between the existing topics and
collected sufficient statistics in <cite>other</cite> to update the topics.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.inference">
<tt class="descname">inference</tt><big>(</big><em>chunk</em>, <em>collect_sstats=False</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a chunk of sparse document vectors, estimate gamma (parameters
controlling the topic weights) for each document in the chunk.</p>
<p>This function does not modify the model (=is read-only aka const). The
whole input chunk of document is assumed to fit in RAM; chunking of a
large corpus must be done earlier in the pipeline.</p>
<p>If <cite>collect_sstats</cite> is True, also collect sufficient statistics needed
to update the model&#8217;s topic-word distributions, and return a 2-tuple
<cite>(gamma, sstats)</cite>. Otherwise, return <cite>(gamma, None)</cite>. <cite>gamma</cite> is of shape
<cite>len(chunk) x topics</cite>.</p>
</dd></dl>

<dl class="classmethod">
<dt id="gensim.models.ldamodel.LdaModel.load">
<em class="property">classmethod </em><tt class="descname">load</tt><big>(</big><em>fname</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a previously saved object from file (also see <cite>save</cite>).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.save">
<tt class="descname">save</tt><big>(</big><em>fname</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the object to file via pickling (also see <cite>load</cite>).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.show_topics">
<tt class="descname">show_topics</tt><big>(</big><em>topics=10</em>, <em>topn=10</em>, <em>log=False</em>, <em>formatted=True</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.show_topics" title="Permalink to this definition">¶</a></dt>
<dd><p>Print the <cite>topN</cite> most probable words for (randomly selected) <cite>topics</cite>
number of topics. Set <cite>topics=-1</cite> to print all topics.</p>
<p>Unlike LSA, there is no ordering between the topics in LDA.
The printed <cite>topics &lt;= self.num_topics</cite> subset of all topics is therefore
arbitrary and may change between two runs.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.update">
<tt class="descname">update</tt><big>(</big><em>corpus</em>, <em>chunksize=None</em>, <em>decay=None</em>, <em>passes=None</em>, <em>update_every=None</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the model with new documents, by EM-iterating over <cite>corpus</cite> until
the topics converge (or until the maximum number of allowed iterations
is reached).</p>
<p>In distributed mode, the E step is distributed over a cluster of machines.</p>
<p>This update also supports updating an already trained model (<cite>self</cite>)
with new documents from <cite>corpus</cite>; the two models are then merged in
proportion to the number of old vs. new documents. This feature is still
experimental for non-stationary input streams.</p>
<p>For stationary input (no topic drift in new documents), on the other hand,
this equals the online update of Hoffman et al. and is guaranteed to
converge for any <cite>decay</cite> in (0.5, 1.0&gt;.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="gensim.models.ldamodel.LdaState">
<em class="property">class </em><tt class="descclassname">gensim.models.ldamodel.</tt><tt class="descname">LdaState</tt><big>(</big><em>eta</em>, <em>shape</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaState" title="Permalink to this definition">¶</a></dt>
<dd><p>Encapsulate information for distributed computation of LdaModel objects.</p>
<p>Objects of this class are sent over the network, so try to keep them lean to
reduce traffic.</p>
<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.blend">
<tt class="descname">blend</tt><big>(</big><em>rhot</em>, <em>other</em>, <em>targetsize=None</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaState.blend" title="Permalink to this definition">¶</a></dt>
<dd><p>Given LdaState <cite>other</cite>, merge it with the current state. Stretch both to
<cite>targetsize</cite> documents before merging, so that they are of comparable
magnitude.</p>
<p>Merging is done by average weighting: in the extremes, <cite>rhot=0.0</cite> means
<cite>other</cite> is completely ignored; <cite>rhot=1.0</cite> means <cite>self</cite> is completely ignored.</p>
<p>This procedure corresponds to the stochastic gradient update from Hoffman
et al., algorithm 2 (eq. 14).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.blend2">
<tt class="descname">blend2</tt><big>(</big><em>rhot</em>, <em>other</em>, <em>targetsize=None</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaState.blend2" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternative, more simple blend.</p>
</dd></dl>

<dl class="classmethod">
<dt id="gensim.models.ldamodel.LdaState.load">
<em class="property">classmethod </em><tt class="descname">load</tt><big>(</big><em>fname</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaState.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a previously saved object from file (also see <cite>save</cite>).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.merge">
<tt class="descname">merge</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaState.merge" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge the result of an E step from one node with that of another node
(summing up sufficient statistics).</p>
<p>The merging is trivial and after merging all cluster nodes, we have the
exact same result as if the computation was run on a single node (no
approximation).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.reset">
<tt class="descname">reset</tt><big>(</big><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaState.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare the state for a new EM iteration (reset sufficient stats).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.save">
<tt class="descname">save</tt><big>(</big><em>fname</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.LdaState.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the object to file via pickling (also see <cite>load</cite>).</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="gensim.models.ldamodel.dirichlet_expectation">
<tt class="descclassname">gensim.models.ldamodel.</tt><tt class="descname">dirichlet_expectation</tt><big>(</big><em>alpha</em><big>)</big><a class="headerlink" href="#gensim.models.ldamodel.dirichlet_expectation" title="Permalink to this definition">¶</a></dt>
<dd><p>For a vector <cite>theta~Dir(alpha)</cite>, compute <cite>E[log(theta)]</cite>.</p>
</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    
        
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="lsimodel.html" title="models.lsimodel – Latent Semantic Indexing"
             >next</a> |</li>
        <li class="right" >
          <a href="../corpora/indexedcorpus.html" title="corpora.indexedcorpus – Random access to corpus documents"
             >previous</a> |</li>
        <li><a href="../index.html">Gensim home</a>|&nbsp;</li>
        <li><a href="../tutorial.html">Tutorials</a>|&nbsp;</li>
        <li><a href="http://groups.google.com/group/gensim">Support</a>|&nbsp;</li>
        <li><a href="https://github.com/piskvorky/gensim/wiki">Contribute</a>|&nbsp;</li>
        <li><a href="../apiref.html">API reference</a>&raquo;</li>

          <li><a href="../apiref.html" >API Reference</a> &raquo;</li> 
      </ul>
    </div>
    

    <div class="footer">
        &copy; Copyright 2012, Radim Řehůřek &lt;radimrehurek(at)seznam.cz&gt;.
      Last updated on Mar 09, 2012.
    </div>
  </body>
</html>